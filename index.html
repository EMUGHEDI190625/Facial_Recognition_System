<!-- 

<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Recognition System</title>

  <!-- Load face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      padding: 10px;
    }
    video, canvas {
      border: 1px solid #ccc;
      border-radius: 8px;
      margin-top: 10px;
    }
    #status {
      margin-bottom: 10px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h3>Face Recognition System</h3>
  <div id="status">Loading face-api.js...</div>

  <input type="text" id="employeeId" placeholder="Enter Employee ID" required><br>
  <input type="text" id="fullName" placeholder="Enter Full Name" required><br>
  <button id="start" disabled>Start Camera</button><br><br>

  <video id="video" width="320" height="240" autoplay muted></video>
  <canvas id="overlay" width="320" height="240"></canvas>

  <script>
    const statusEl = document.getElementById("status");
    const startBtn = document.getElementById("start");
    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");
    let labeledDescriptors = [];

    // ‚úÖ Path to your model and image folders
    const MODEL_URL = "./model/weights/";
    const IMAGE_PATH = "./images/"; // your images folder

    // ‚úÖ Function to send recognition data to Google Sheet
    async function sendToSheet(employeeId, fullName, imageLink) {
      const SHEET_URL = "https://script.google.com/macros/s/AKfycbzY0QiXZq23BrmqXMtkRnmptg59V11XrjQDNp9j0Lv9J1QrELWBAitSTClW8AmOK8yTQA/exec"; // üîó Replace with your Apps Script Web App URL

      const data = {
        employeeId,
        fullName,
        imageLink
      };

      try {
        await fetch(SHEET_URL, {
          method: "POST",
          body: JSON.stringify(data),
          headers: { "Content-Type": "application/json" },
        });
        console.log("‚úÖ Data sent to Google Sheet");
      } catch (err) {
        console.error("‚ùå Error sending to sheet:", err);
      }
    }

    // ‚úÖ Load face-api.js models first
    async function loadModels() {
      try {
        statusEl.textContent = "‚è≥ Loading models...";

        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
          faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
        ]);

        statusEl.textContent = "‚úÖ Models loaded ‚Äî enter your name and start camera.";
        startBtn.disabled = false;
      } catch (err) {
        statusEl.textContent = "‚ùå Model load error: " + err.message;
        console.error(err);
      }
    }

    // ‚úÖ Load the user's image based on the name typed
    async function loadLabeledImage(fullName) {
      labeledDescriptors = [];
      const imageName = fullName.trim(); 
      const imgUrl = `${IMAGE_PATH}${imageName}.jpg`;

      try {
        const img = await faceapi.fetchImage(imgUrl);
        const detections = await faceapi
          .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceDescriptor();

        if (detections) {
          labeledDescriptors.push(
            new faceapi.LabeledFaceDescriptors(fullName, [detections.descriptor])
          );
          console.log(`‚úÖ Loaded ${fullName}'s face`);
        } else {
          console.warn(`‚ö†Ô∏è No face detected for ${fullName}`);
        }
      } catch (err) {
        console.error(`Error loading image for ${fullName}:`, err);
        alert(`‚ùå Could not load image for "${fullName}". Make sure ${imageName}.jpg exists in /images folder.`);
      }
    }

    // ‚úÖ Start camera and recognition
    startBtn.onclick = async () => {
      const fullName = document.getElementById("fullName").value.trim();
      const employeeId = document.getElementById("employeeId").value.trim();

      if (!fullName || !employeeId) {
        alert("Please enter both Employee ID and Full Name before starting.");
        return;
      }

      await loadLabeledImage(fullName);
      if (!labeledDescriptors || labeledDescriptors.length === 0) {
        alert("No labeled face loaded! Check image name or path.");
        return;
      }

      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
        statusEl.textContent = "üé• Camera started ‚Äî detecting faces...";

        const faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);

        setInterval(async () => {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptors();

          ctx.clearRect(0, 0, canvas.width, canvas.height);
          faceapi.matchDimensions(canvas, { width: video.width, height: video.height });

          const resized = faceapi.resizeResults(detections, {
            width: video.width,
            height: video.height
          });

          const results = resized.map(d => faceMatcher.findBestMatch(d.descriptor));

          // ‚úÖ Modified loop: Send recognized data to Google Sheet
          results.forEach(async (result, i) => {
            const box = resized[i].detection.box;
            const label = result.toString();
            const drawBox = new faceapi.draw.DrawBox(box, { label });
            drawBox.draw(canvas);

            if (!label.includes("unknown")) {
              const imageLink  = `${location.origin}/images/${fullName}.jpg`;
              statusEl.textContent = `‚úÖ Face recognized: ${fullName}`;
              await sendToSheet(employeeId, fullName, imageLink);
            }
          });
        }, 1000);
      } catch (err) {
        statusEl.textContent = "‚ö†Ô∏è Camera error: " + err.message;
        console.error(err);
      }
    };

    window.addEventListener("load", loadModels);
  </script>
</body>
</html>
 -->


<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ECN Face Recognition System</title>

  <!-- Load face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      padding: 10px;
      background: #f4f4f4;
    }
    video, canvas {
      border: 2px solid #000;
      border-radius: 8px;
      margin-top: 10px;
    }
    input, button {
      margin: 5px;
      padding: 8px;
      font-size: 16px;
    }
    #status {
      margin-top: 10px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h2>ECN Face Recognition System</h2>

  <input type="text" id="employeeId" placeholder="Enter Employee ID" required><br>
  <button id="startBtn" disabled>Start Camera</button>

  <p id="status">Loading models...</p>

  <video id="video" width="320" height="240" autoplay muted></video>
  <canvas id="overlay" width="320" height="240"></canvas>

  <script>
    const statusEl = document.getElementById("status");
    const startBtn = document.getElementById("startBtn");
    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");

    let labeledDescriptors = [];
    let faceMatcher;

    // Your Google Apps Script Web App URL
    const SHEET_URL = "https://script.google.com/macros/s/AKfycbzY0QiXZq23BrmqXMtkRnmptg59V11XrjQDNp9j0Lv9J1QrELWBAitSTClW8AmOK8yTQA/exec";

    // Load face-api.js models from CDN
    async function loadModels() {
      statusEl.textContent = "‚è≥ Loading models...";
      try {
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models/'),
          faceapi.nets.faceLandmark68Net.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models/'),
          faceapi.nets.faceRecognitionNet.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models/')
        ]);
        statusEl.textContent = "‚úÖ Models loaded. Enter Employee ID and start camera.";
        startBtn.disabled = false;
      } catch(err) {
        statusEl.textContent = "‚ùå Error loading models: " + err;
        console.error(err);
      }
    }

    // Load all authorized faces from /images folder
    async function loadAuthorizedFaces() {
      labeledDescriptors = [];
      const authorizedNames = ["John Doe", "Jane Smith", "Ikata Kenoye"]; // replace with your employees
      for (const name of authorizedNames) {
        try {
          const img = await faceapi.fetchImage(`./images/${name}.jpg`);
          const detection = await faceapi
            .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptor();

          if (detection) {
            labeledDescriptors.push(
              new faceapi.LabeledFaceDescriptors(name, [detection.descriptor])
            );
            console.log(`‚úÖ Loaded authorized face for ${name}`);
          } else {
            console.warn(`‚ö†Ô∏è No face detected in ${name}.jpg`);
          }
        } catch (err) {
          console.error(err);
        }
      }

      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
    }

    // Send captured image to Google Sheet
    async function sendToSheet(employeeId, imageData, status) {
      const data = {
        employeeId,
        imageLink: imageData,
        status,
        timestamp: new Date().toLocaleString()
      };

      try {
        await fetch(SHEET_URL, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(data)
        });
        console.log("‚úÖ Data sent to Google Sheet:", data);
      } catch(err) {
        console.error("‚ùå Error sending to Google Sheet:", err);
      }
    }

    // Start webcam and recognition
    startBtn.onclick = async () => {
      const employeeId = document.getElementById("employeeId").value.trim();
      if (!employeeId) {
        alert("Please enter Employee ID.");
        return;
      }

      await loadAuthorizedFaces();

      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
        statusEl.textContent = "üé• Camera started ‚Äî detecting faces...";

        setInterval(async () => {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptors();

          ctx.clearRect(0, 0, canvas.width, canvas.height);
          faceapi.matchDimensions(canvas, { width: video.width, height: video.height });
          const resized = faceapi.resizeResults(detections, { width: video.width, height: video.height });

          const results = resized.map(d => faceMatcher.findBestMatch(d.descriptor));

          for (let i = 0; i < results.length; i++) {
            const result = results[i];
            const box = resized[i].detection.box;
            const drawBox = new faceapi.draw.DrawBox(box, { label: result.label === "unknown" ? "Unauthorized" : "Authorized" });
            drawBox.draw(canvas);

            // Capture frame
            const captureCanvas = document.createElement("canvas");
            captureCanvas.width = video.videoWidth;
            captureCanvas.height = video.videoHeight;
            captureCanvas.getContext("2d").drawImage(video, 0, 0, captureCanvas.width, captureCanvas.height);
            const imageDataURL = captureCanvas.toDataURL("image/jpeg");

            const statusText = result.label === "unknown" ? "Unauthorized" : "Authorized";
            statusEl.textContent = `Face detected: ${statusText}`;

            // Send to Google Sheet
            await sendToSheet(employeeId, imageDataURL, statusText);
          }
        }, 1500); // every 1.5 sec
      } catch(err) {
        statusEl.textContent = "‚ö†Ô∏è Camera error: " + err.message;
        console.error(err);
      }
    };

    window.addEventListener("load", loadModels);
  </script>
</body>
</html>
